air_time_diff = air_time2 - air_time) %>%
filter(air_time_diff != 0) %>%
select(air_time, air_time2, dep_time, arr_time, dest)
time2min <- function(x){}
time2min <- function(x){
x %/% 100 * 60 + x %% 100
}
mutate(flights,
dep_delay2 = time2min(dep_time) - time2min(sched_dep_time)) %>%
filter(dep_delay2 != dep_delay) %>%
select(dep_time, sched_dep_time, dep_delay, dep_delay2)
?min_rank
mutate(flights,
dep_delay_rank = min_rank(dep_delay)) %>%
arrange(dep_delay_rank) %>%
filter(dep_delay_rank <= 10)
dep_delay_rank = min_rank(-dep_delay)) %>%
mutate(flights,
dep_delay_rank = min_rank(-dep_delay)) %>%
arrange(dep_delay_rank) %>%
filter(dep_delay_rank <= 10)
summarise(flights, delay = mean(dep_delay, na.rm = TRUE))
by_day <- group_by(flights, year, month, day)
summarise(by_day, delay = mean(dep_delay, na.rm = TRUE))
#
delays <- flights %>%
group_by(dest) %>%
summarise(
count = n(),
dist = mean(distance, na.rm = TRUE),
delay = mean(arr_delay, na.rm = TRUE)
)
filter(delay, count > 20, dest  != "HNL")
filter(delays, count > 20, dest  != "HNL")
not_cancelled <- flights %>%
filter(!is.na(dep_delay), !is.na(arr_delay))
not_cancelled %>%
group_by(year, month, day) %>%
summarise(mean = mean(dep_delay))
delays <- not_cancelled %>%
group_by(tailnum) %>%
summarise(
delay = mean(arr_delay)
)
ggplot(data=delay, mapping = aes(x=delay)) + geom_freqpoly(binwidth = 10)
ggplot(data=delays, mapping = aes(x=delay)) + geom_freqpoly(binwidth = 10)
delays <- not_cancelled %>%
group_by(tailnum) %>%
summarise(
delay = mean(arr_delay, na.rm = TRUE),
n = n()
)
ggplot(data=delays, mapping = aes(x=n, y=delay)) + geom_point(alpha = 1/10)
delays %>%
filter(n > 25) %>%
ggplot(mapping = aes(x = n, y = delay)) +
geom_point(alpha = 1/10)
install.packages("rmarkdown")
library(rmarkdown)
plot(cars)
plot(cars)
# Chunk 1
plot(cars)
# Chunk 1
plot(cars)
plot(cars)
library(nycflights13)
library(tidyverse)
plot(cars)
library(nycflights13)
library(tidyverse)
install.packages("twitteR")
install.packages("ROAuth")
install.packages("stringr")
install.packages("tm")
install.packages("wordcloud")
library(wordcloud)
library(twitteR)
library(ROAuth)
library(stringr)
library(tm)
library(wordcloud)
library(twitteR)
library(ROAuth)
library(stringr)
library(tm)
library(wordcloud)
load("twitter authentication.Rdata")
download.file(url="http://curl.haxx.se/ca/cacert.pem",destfile="cacert.pem")
cred <- OAuthFactory$new(consumerKey='mftPn6qtOfCL46fzJY3pTeVI4',
consumerSecret='x5D1VIlV1O4TkZPshj31j88CutI8X2eSfgryy0pdMpDJjUTcKK',
requestURL='https://api.twitter.com/oauth/request_token',
accessURL='https://api.twitter.com/oauth/access_token',
authURL='https://api.twitter.com/oauth/authorize')
cred$handshake(cainfo="cacert.pem")
cred <- OAuthFactory$new(consumerKey='mftPn6qtOfCL46fzJY3pTeVI4',
+                          consumerSecret='x5D1VIlV1O4TkZPshj31j88CutI8X2eSfgryy0pdMpDJjUTcKK',
+                          requestURL='https://api.twitter.com/oauth/request_token',
+                          accessURL='https://api.twitter.com/oauth/access_token',
+                          authURL='https://api.twitter.com/oauth/authorize')
> cred$handshake(cainfo="cacert.pem")
cred <- OAuthFactory$new(consumerKey='mftPn6qtOfCL46fzJY3pTeVI4',
consumerSecret='x5D1VIlV1O4TkZPshj31j88CutI8X2eSfgryy0pdMpDJjUTcKK',
requestURL='https://api.twitter.com/oauth/request_token',
accessURL='https://api.twitter.com/oauth/access_token',
authURL='https://api.twitter.com/oauth/authorize')
cred$handshake(cainfo="cacert.pem")
datascience <- searchTwitter(#datascience, n=2000, lang=NULL)
datascience <- searchTwitter('#datascience', n=2000, lang='en')
setup_twitter_oauth(mftPn6qtOfCL46fzJY3pTeVI4,
x5D1VIlV1O4TkZPshj31j88CutI8X2eSfgryy0pdMpDJjUTcKK,
2924790128-WORx3u9bTwDqAYr6zSanDnDlafLaSHBdzVKYZxC,
aMFTRClgP3Q2K0pJ21p1fromq1OWx4cS1hU6i4qJyqpZb
)
requestURL <- "https://api.twitter.com/oauth/request_token"
accessURL <- "https://api.twitter.com/oauth/access_token"
authURL <- "https://api.twitter.com/oauth/authorize"
setup_twitter_oauth(mftPn6qtOfCL46fzJY3pTeVI4,
x5D1VIlV1O4TkZPshj31j88CutI8X2eSfgryy0pdMpDJjUTcKK,
2924790128-WORx3u9bTwDqAYr6zSanDnDlafLaSHBdzVKYZxC,
aMFTRClgP3Q2K0pJ21p1fromq1OWx4cS1hU6i4qJyqpZb
)
setup_twitter_oauth(mftPn6qtOfCL46fzJY3pTeVI4,
x5D1VIlV1O4TkZPshj31j88CutI8X2eSfgryy0pdMpDJjUTcKK)
library(twitteR)
library(ROAuth)
library(stringr)
library(tm)
library(wordcloud)
consumer_key <- "mftPn6qtOfCL46fzJY3pTeVI4"
consumer_secret <- "x5D1VIlV1O4TkZPshj31j88CutI8X2eSfgryy0pdMpDJjUTcKK"
access_token <- "2924790128-WORx3u9bTwDqAYr6zSanDnDlafLaSHBdzVKYZxC"
access_secret <- "aMFTRClgP3Q2K0pJ21p1fromq1OWx4cS1hU6i4qJyqpZb"
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
test <- userTimeline(user = 'dsilvadeepal', n=1000)
length(test)
View(test)
screenName <- c(‘Benioff’, ‘aplusk’, ‘tim_cook’, 'sundarpichai')
screenName <- c("Benioff")
screenName <- c("Benioff", "aplusk", "tim_cook", "sundarpichai")
checkHandles <- lookupUsers(screenName)
UserData <- lapply(checkHandles, function(x) getUser(x))
UserData <- twListToDF(UserData)
View(UserData)
UserData <- lapply(checkHandles, function(x) getUser(x))
UserData <- twListToDF(UserData)
table(UserData$name, UserData$statusesCount)
table(UserData$name, UserData$protected)
usernames <- subset(UserData, protected == FALSE)
View(usernames)
View(usernames)
usernames <- as.list(usernames$screenName)
x <- userTimeline(‘Benioff’,n=3200,includeRts = FALSE)
x <- userTimeline("Benioff",n=4200,includeRts = FALSE)
x <- userTimeline("Benioff",n=3200,includeRts = FALSE)
BenioffData <- twListToDF(x)
Sys.sleep(300)
x <- userTimeline("aplusk",n=3200,includeRts = FALSE)
AshtonData <- twListToDF(x)
Benioff.df <- data.frame(BenioffData)
Ashton.df <-  data.frame(AshtonData)
tweets <- data.frame()
tweets <- rbind(Benioff.df, Ashton.df)
iconv(tweets$text, from=”UTF-8", to=”ASCII”, sub=””)
iconv(tweets$text, from="UTF-8", to=”ASCII”, sub=””)
iconv(tweets$text, from="UTF-8", to="ASCII", sub="")
tweets$text=str_replace_all(tweets$text,"[^[:graph:]]", " ")
tweets$text <- gsub("[^[:alnum:]///' ]", "", tweets$text)
tweets$text <- tolower(tweets$text)
tweets$text <- gsub("rt", "", tweets$text)
tweets$text <- gsub("[[:punct:]]", "", tweets$text)
tweets$text <- gsub("http\\w+", "", tweets$text)
tweets$text <- gsub("[ |\t]{2,}", "", tweets$text)
tweets$text <- gsub("^ ", "", tweets$text)
tweets$text <- gsub(" $", "", tweets$text)
tweets$text <- gsub("@\\w+", "", tweets$text)
Benioff <- subset(tweets, screenName=="Benioff", select= text)
Ashton <- subset(tweets, screenName=="aplusk", select= text)
Benioff <- Corpus(VectorSource(Benioff))
Ashton <- Corpus(VectorSource(Ashton))
Benioff <- tm_map(Benioff, removeWords, stopwords("en"))
Ashton <- tm_map(Ashton, removeWords, stopwords("en"))
wordcloud(Benioff,min.freq = 3, scale=c(6,0.5),colors=brewer.pal(8, "Dark2"),random.color= FALSE, random.order = FALSE, max.words = 110)
wordcloud(Benioff,min.freq = 4, scale=c(6,0.5),colors=brewer.pal(8, "Dark2"),random.color= FALSE, random.order = FALSE, max.words = 110)
wordcloud(Benioff,min.freq = 2, scale=c(6,0.5),colors=brewer.pal(8, "Dark2"),random.color= FALSE, random.order = FALSE, max.words = 110)
wordcloud(Ashton,min.freq = 4, scale=c(7,0.8),colors=brewer.pal(8, "Dark2"),random.color= FALSE, random.order = FALSE, max.words = 100)
wordcloud(Ashton,min.freq = 4, scale=c(3,0.8),colors=brewer.pal(8, "Dark2"),random.color= FALSE, random.order = FALSE, max.words = 100)
wordcloud(Ashton,min.freq = 4, scale=c(6,0.4),colors=brewer.pal(8, "Dark2"),random.color= FALSE, random.order = FALSE, max.words = 100)
wordcloud(Ashton,min.freq = 4, scale=c(3,0.4),colors=brewer.pal(8, "Dark2"),random.color= FALSE, random.order = FALSE, max.words = 100)
wordcloud(Benioff,min.freq = 4, scale=c(3,0.4),colors=brewer.pal(8, "Dark2"),random.color= FALSE, random.order = FALSE, max.words = 100)
wordcloud(Benioff,min.freq = 2, scale=c(3,0.4),colors=brewer.pal(8, "Dark2"),random.color= FALSE, random.order = FALSE, max.words = 100)
install.packages("swirl")
warnings()
library("swirl")
swirl()
bye()
Prediction5 <- predict(fit, test)
#Deepal DSilva  2/6/2018
#http://trevorstephens.com/kaggle-titanic-tutorial/r-part-1-booting-up/
#http://trevorstephens.com/kaggle-titanic-tutorial/r-part-2-the-gender-class-model/
# Set working directory and import datafiles
setwd("C:/Users/dsilv/Desktop/Learning/Data Science/Data-Science/Kaggle/Titanic")
train <- read.csv("C:/Users/dsilv/Desktop/Learning/Data Science/Data-Science/Kaggle/Titanic/train.csv")
test <- read.csv("C:/Users/dsilv/Desktop/Learning/Data Science/Data-Science/Kaggle/Titanic/test.csv")
#All perish prediction
submit <- data.frame(PassengerId = test$PassengerId, Survived = test$Survived)
write.csv(submit, file = "theyallperish.csv", row.names = FALSE)
#Gender Age factor
summary(train$Sex)
prop.table(table(train$Sex, train$Survived),1)
test$Survived <- 0
test$Survived[test$Sex == 'female'] <- 1
submit <- data.frame(PassengerId = test$PassengerId, Survived = test$Survived)
write.csv(submit, file = "onlyfemalessurvive.csv", row.names = FALSE)
aggregate(Survived ~ Child + Sex, data=train, FUN=function(x) {sum(x)/length(x)})
train$Fare2 <- '30+'
train$Fare2[train$Fare < 30 & train$Fare >= 20] <- '20-30'
train$Fare2[train$Fare < 20 & train$Fare >= 10] <- '10-20'
train$Fare2[train$Fare < 10] <- '<10'
table(train$Fare2)
aggregate(Survived ~ Fare2 + Pclass + Sex, data=train, FUN=function(x) {sum(x)/length(x)})
#Import rpart i.e. Recursive Partitioning and Regression Trees
#Using Decision Trees
library(rpart)
fit <- rpart(Survived ~ Pclass + Sex + Age +  SibSp + Parch + Fare + Embarked, data=train, method="class")
library(rpart.plot)
library(RColorBrewer)
library(rattle)
fancyRpartPlot(fit)
Prediction <- predict(fit, test, type = "class")
submit <- data.frame(PassengerId = test$PassengerId, Survived = Prediction)
write.csv(submit, file = "myfirstdtree.csv", row.names = FALSE)
#Manually trim a decision tree
fit <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, data=train,
method="class", control=rpart.control(minsplit=2, cp=0.005))
fancyRpartPlot(fit)
Prediction1 <- predict(fit, test, type = "class")
submit <- data.frame(PassengerId = test$PassengerId, Survived = Prediction1)
write.csv(submit, file = "mysnippedtree.csv", row.names = FALSE)
#Feature Engineering
test$Survived <- NA
combi <- rbind(train,test)
#Cast Name column back to a text string not as levels
train$Name[1]
combi$Name <- as.character(combi$Name)
combi$Name[1]
strsplit(combi$Name[1], split='[,.]')[[1]][2]
combi$Title <- sapply(combi$Name, FUN=function(x){strsplit(x, split='[,.]')[[1]][2]})
combi$Title <- sub(' ', '', combi$Title)
table(combi$Title)
combi$Title[combi$T %in% c('Mme', 'Mlle')] <- 'Mlle'
combi$Title[combi$T %in% c('Capt', 'Don', 'Major', 'Sir', 'Jonkheer')] <- 'Sir'
combi$Title[combi$T %in% c('Dona', 'Lady', 'the Countess')] <- 'Lady'
combi$Title <- factor(combi$Title)
combi$FamilySize <- combi$SibSp + combi$Parch + 1
combi$Surname <- sapply(combi$Name, FUN=function(x){strsplit(x, split='[,.]')[[1]][1]})
combi$FamilyId <- paste(as.character(combi$FamilySize), combi$Surname, sep = "")
combi$FamilyId[combi$FamilySize <2] <- 'Small'
table(combi$FamilyId)
famIDs <- data.frame(table(combi$FamilyId))
famIDs <- famIDs[famIDs$Freq <= 2, ]
combi$FamilyId[combi$FamilyId %in% famIDs$Var1] <- 'Small'
combi$FamilyId <- factor(combi$FamilyId)
#Split data back to train & test data
train <- combi[1:891,]
test <- combi[892:1309,]
fit <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare +
Embarked + Title + FamilySize + FamilyId,
data=train, method="class")
fancyRpartPlot(fit)
Prediction2 <- predict(fit, test, type = "class")
submit <- data.frame(PassengerId = test$PassengerId, Survived = Prediction2)
write.csv(submit, file = "featureenggtree.csv", row.names = FALSE)
#Random forests
#Fill in the missing Age values
summary(combi$Age)
Agefit <- rpart(Age ~ Pclass + Sex + SibSp + Parch + Fare + Embarked + Title + FamilySize,
data=combi[!is.na(combi$Age),],
method="anova")
combi$Age[is.na(combi$Age)] <- predict(Agefit, combi[is.na(combi$Age),])
which(combi$Embarked == '')
combi$Embarked[c(62, 830)] = 'S'
combi$Embarked <- factor(combi$Embarked)
summary(combi$Fare)
which(is.na(combi$Fare))
combi$Fare[1044] <- median(combi$Fare, na.rm = TRUE)
combi$FamilyID2 <- combi$FamilyId
combi$FamilyID2 <- as.character(combi$FamilyID2)
combi$FamilyID2[combi$FamilySize <= 3] <- 'Small'
combi$FamilyID2 <- factor(combi$FamilyID2)
#Split data back to train & test data
train <- combi[1:891,]
test <- combi[892:1309,]
library(randomForest)
set.seed(400)
fit <- randomForest(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + Fare +
Embarked + Title + FamilySize + FamilyID2,
data=train,
importance=TRUE,
ntree=2500)
varImpPlot(fit)
Prediction5 <- predict(fit, test)
submit <- data.frame(PassengerId = test$PassengerId, Survived = Prediction5)
write.csv(submit, file = "firstforest.csv", row.names = FALSE)
library(swirl)
swirl(swirl)
swirl()
5 + 7
x <- 5 + 6
x <- 5 + 7
x
y <-  x - 3
y
z <-  c(1.1, 9. 3.14)
z <- c(1.1, 9. 3.14)
z <- c(1.1, 9. 3.14)
z <- c(1.1, 9, 3.14)
?c
z
c(z, 555, z)
z * 2 + 100
my_sqrt <- sqrt(z - 1)
my_sqrt
my_div <- z / my_sqrt
my_div
c(1,2,3,4) + c(0,10)
c(1,2,3,4) + c(0,10, 100)
c(1,2,3,4) + c(0,10,100)
z * 2 + 1000
my_div
ls()
info()
bye()
library(twitteR)
library(ROAuth)
library(stringr)
library(tm)
library(wordcloud)
consumer_key <- "mftPn6qtOfCL46fzJY3pTeVI4"
consumer_secret <- "x5D1VIlV1O4TkZPshj31j88CutI8X2eSfgryy0pdMpDJjUTcKK"
access_token <- "2924790128-WORx3u9bTwDqAYr6zSanDnDlafLaSHBdzVKYZxC"
access_secret <- "aMFTRClgP3Q2K0pJ21p1fromq1OWx4cS1hU6i4qJyqpZb"
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
screenName <- c("katyperry", "justinbieber", "rihanna", "taylorswift13", "TheEllenShow")
checkHandles <- lookupUsers(screenName)
x <- userTimeline("katyperry",n=3200,includeRts = FALSE)
KPerryData <- twListToDF(x)
KPerry.df <- data.frame(KPerryData)
unclean_tweets <- data.frame()
unclean_tweets <- KPerry.df
clean_tweet = gsub("&amp", "", unclean_tweets)
clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet)
clean_tweet = gsub("@\\w+", "", clean_tweet)
clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
clean_tweet = gsub("http\\w+", "", clean_tweet)
clean_tweet = gsub("[ \t]{2,}", "", clean_tweet)
clean_tweet = gsub("^\\s+|\\s+$", "", clean_tweet)
KatyPerry <- Corpus(VectorSource(clean_tweet))
KatyPerry <- tm_map(KatyPerry, removeWords, stopwords(“en”))
KatyPerry <- tm_map(KatyPerry, removeWords, stopwords("en"))
wordcloud(KatyPerry,min.freq = 3, scale=c(6,0.5),colors=brewer.pal(8, "Dark2"),random.color= FALSE, random.order = FALSE, max.words = 110)
wordcloud(KatyPerry,min.freq = 3, scale=c(6,0.7),colors=brewer.pal(8, "Dark2"),random.color= FALSE, random.order = FALSE, max.words = 110)
clean_tweet = gsub("&amp", "", unclean_tweets)
clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet)
clean_tweet = gsub("@\\w+", "", clean_tweet)
clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
#clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
clean_tweet = gsub("http\\w+", "", clean_tweet)
clean_tweet = gsub("[ \t]{2,}", "", clean_tweet)
clean_tweet = gsub("^\\s+|\\s+$", "", clean_tweet)
KatyPerry <- Corpus(VectorSource(clean_tweet))
KatyPerry <- tm_map(KatyPerry, removeWords, stopwords("en"))
wordcloud(KatyPerry,min.freq = 3, scale=c(6,0.7),colors=brewer.pal(8, "Dark2"),random.color= FALSE, random.order = FALSE, max.words = 110)
clean_tweet = gsub("&amp", "", unclean_tweets)
clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet)
clean_tweet = gsub("@\\w+", "", clean_tweet)
clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
clean_tweet = gsub("http\\w+", "", clean_tweet)
clean_tweet = gsub("[ \t]{2,}", "", clean_tweet)
clean_tweet = gsub("^\\s+|\\s+$", "", clean_tweet)
KatyPerry <- Corpus(VectorSource(clean_tweet))
KatyPerry <- tm_map(KatyPerry, removeWords, stopwords("en"))
wordcloud(KatyPerry,min.freq = 3, scale=c(6,0.7),colors=brewer.pal(8, "Dark2"),random.color= FALSE, random.order = FALSE, max.words = 110)
wordcloud(KatyPerry,min.freq = 4, scale=c(7,0.7),colors=brewer.pal(8, "Dark2"),random.color= FALSE, random.order = FALSE, max.words = 110)
wordcloud(KatyPerry,min.freq = 5, scale=c(7,0.7),colors=brewer.pal(8, "Dark2"),random.color= FALSE, random.order = FALSE, max.words = 110)
x <- userTimeline("justinbieber",n=3200,includeRts = FALSE)
KPerryData <- twListToDF(x)
KPerry.df <- data.frame(KPerryData)
unclean_tweets <- data.frame()
unclean_tweets <- KPerry.df
clean_tweet = gsub("&amp", "", unclean_tweets)
clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet)
clean_tweet = gsub("@\\w+", "", clean_tweet)
clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
clean_tweet = gsub("http\\w+", "", clean_tweet)
clean_tweet = gsub("[ \t]{2,}", "", clean_tweet)
clean_tweet = gsub("^\\s+|\\s+$", "", clean_tweet)
KatyPerry <- Corpus(VectorSource(clean_tweet))
KatyPerry <- tm_map(KatyPerry, removeWords, stopwords("en"))
wordcloud(KatyPerry,min.freq = 5, scale=c(7,0.7),colors=brewer.pal(8, "Dark2"),random.color= FALSE, random.order = FALSE, max.words = 110)
iconv(unclean_tweets, from="UTF-8", to="ASCII", sub="")
clean_tweet = str_replace_all(unclean_tweets,"[^[:graph:]]", " ")
clean_tweet <- gsub("[^[:alnum:]///' ]", "", clean_tweet)
iconv(unclean_tweets, from="UTF-8", to="ASCII", sub="")
clean_tweet <-  str_replace_all(unclean_tweets,"[^[:graph:]]", " ")
clean_tweet <- gsub("[^[:alnum:]///' ]", "", clean_tweet)
clean_tweet = gsub("&amp", "", clean_tweet)
clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet)
clean_tweet <- tolower(clean_tweet)
clean_tweet = gsub("@\\w+", "", clean_tweet)
clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
clean_tweet = gsub("http\\w+", "", clean_tweet)
clean_tweet = gsub("[ \t]{2,}", "", clean_tweet)
clean_tweet = gsub("^\\s+|\\s+$", "", clean_tweet)
KatyPerry <- Corpus(VectorSource(clean_tweet))
KatyPerry <- tm_map(KatyPerry, removeWords, stopwords("en"))
wordcloud(KatyPerry,min.freq = 5, scale=c(7,0.7),colors=brewer.pal(8, "Dark2"),random.color= FALSE, random.order = FALSE, max.words = 110)
x <- userTimeline("TheEllenShow",n=3200,includeRts = FALSE)
KPerryData <- twListToDF(x)
KPerry.df <- data.frame(KPerryData)
unclean_tweets <- data.frame()
unclean_tweets <- KPerry.df
iconv(unclean_tweets, from="UTF-8", to="ASCII", sub="")
clean_tweet <-  str_replace_all(unclean_tweets,"[^[:graph:]]", " ")
clean_tweet <- gsub("[^[:alnum:]///' ]", "", clean_tweet)
clean_tweet = gsub("&amp", "", clean_tweet)
clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet)
clean_tweet <- tolower(clean_tweet)
clean_tweet = gsub("@\\w+", "", clean_tweet)
clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
clean_tweet = gsub("http\\w+", "", clean_tweet)
clean_tweet = gsub("[ \t]{2,}", "", clean_tweet)
clean_tweet = gsub("^\\s+|\\s+$", "", clean_tweet)
KatyPerry <- Corpus(VectorSource(clean_tweet))
KatyPerry <- tm_map(KatyPerry, removeWords, stopwords("en"))
wordcloud(KatyPerry,min.freq = 5, scale=c(7,0.7),colors=brewer.pal(8, "Dark2"),random.color= FALSE, random.order = FALSE, max.words = 110)
tm_map(KatyPerry, removeWords, c("false", "relnofollowtwitter"))
KatyPerry <- Corpus(VectorSource(clean_tweet))
KatyPerry <- tm_map(KatyPerry, removeWords, stopwords("en"))
wordcloud(KatyPerry,min.freq = 5, scale=c(7,0.7),colors=brewer.pal(8, "Dark2"),random.color= FALSE, random.order = FALSE, max.words = 110)
tm_map(clean_tweet, removeWords, c("false", "relnofollowtwitter"))
KatyPerry <- Corpus(VectorSource(clean_tweet))
KatyPerry <- tm_map(KatyPerry, removeWords, stopwords("en"))
wordcloud(KatyPerry,min.freq = 5, scale=c(7,0.7),colors=brewer.pal(8, "Dark2"),random.color= FALSE, random.order = FALSE, max.words = 110)
KPerryData <- twListToDF(x)
KPerry.df <- data.frame(KPerryData)
unclean_tweets <- data.frame()
unclean_tweets <- KPerry.df
iconv(unclean_tweets, from="UTF-8", to="ASCII", sub="")
clean_tweet <-  str_replace_all(unclean_tweets,"[^[:graph:]]", " ")
clean_tweet <- gsub("[^[:alnum:]///' ]", "", clean_tweet)
clean_tweet = gsub("&amp", "", clean_tweet)
clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet)
clean_tweet <- tolower(clean_tweet)
clean_tweet = gsub("@\\w+", "", clean_tweet)
clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
clean_tweet = gsub("http\\w+", "", clean_tweet)
clean_tweet = gsub("[ \t]{2,}", "", clean_tweet)
clean_tweet = gsub("^\\s+|\\s+$", "", clean_tweet)
tm_map(clean_tweet, removeWords, c("false", "relnofollowtwitter", "href", "web", "clienta"))
KatyPerry <- Corpus(VectorSource(clean_tweet))
KatyPerry <- tm_map(KatyPerry, removeWords, stopwords("en"))
wordcloud(KatyPerry,min.freq = 5, scale=c(7,0.7),colors=brewer.pal(8, "Dark2"),random.color= FALSE, random.order = FALSE, max.words = 110)
KPerryData <- twListToDF(x)
KPerry.df <- data.frame(KPerryData)
unclean_tweets <- data.frame()
unclean_tweets <- KPerry.df
iconv(unclean_tweets, from="UTF-8", to="ASCII", sub="")
clean_tweet <-  str_replace_all(unclean_tweets,"[^[:graph:]]", " ")
clean_tweet <- gsub("[^[:alnum:]///' ]", "", clean_tweet)
tm_map(clean_tweet, removeWords, c("false", "relnofollowtwitter", "href", "web", "clienta"))
clean_tweet = gsub("&amp", "", clean_tweet)
clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet)
clean_tweet <- tolower(clean_tweet)
clean_tweet = gsub("@\\w+", "", clean_tweet)
clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
clean_tweet = gsub("http\\w+", "", clean_tweet)
clean_tweet = gsub("[ \t]{2,}", "", clean_tweet)
clean_tweet = gsub("^\\s+|\\s+$", "", clean_tweet)
KatyPerry <- Corpus(VectorSource(clean_tweet))
KatyPerry <- tm_map(KatyPerry, removeWords, stopwords("en"))
wordcloud(KatyPerry,min.freq = 5, scale=c(7,0.7),colors=brewer.pal(8, "Dark2"),random.color= FALSE, random.order = FALSE, max.words = 110)
screenName <- c("katyperry", "justinbieber", "rihanna", "taylorswift13", "TheEllenShow")
checkHandles <- lookupUsers(screenName)
x <- userTimeline("taylorswift13",n=3200,includeRts = FALSE)
KPerryData <- twListToDF(x)
KPerry.df <- data.frame(KPerryData)
unclean_tweets <- data.frame()
unclean_tweets <- KPerry.df
iconv(unclean_tweets, from="UTF-8", to="ASCII", sub="")
clean_tweet <-  str_replace_all(unclean_tweets,"[^[:graph:]]", " ")
clean_tweet <- gsub("[^[:alnum:]///' ]", "", clean_tweet)
tm_map(clean_tweet, removeWords, c("false", "relnofollowtwitter", "href", "web", "clienta"))
clean_tweet = gsub("&amp", "", clean_tweet)
clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet)
clean_tweet <- tolower(clean_tweet)
clean_tweet = gsub("@\\w+", "", clean_tweet)
clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
clean_tweet = gsub("http\\w+", "", clean_tweet)
clean_tweet = gsub("[ \t]{2,}", "", clean_tweet)
clean_tweet = gsub("^\\s+|\\s+$", "", clean_tweet)
KatyPerry <- Corpus(VectorSource(clean_tweet))
KatyPerry <- tm_map(KatyPerry, removeWords, stopwords("en"))
wordcloud(KatyPerry,min.freq = 5, scale=c(7,0.7),colors=brewer.pal(8, "Dark2"),random.color= FALSE, random.order = FALSE, max.words = 110)
