wordcloud(KatyPerry, min.freq = 2, scale=c(6,0.5),colors=brewer.pal(12, "Paired"), random.order = FALSE, max.words = 220)
wordcloud(KatyPerry, min.freq = 2, scale=c(6,0.5),colors=brewer.pal(8, "Accent"), random.order = FALSE, max.words = 220)
KatyPerry <- Corpus(VectorSource(clean_tweet))
KatyPerry <- tm_map(KatyPerry, tolower)
KatyPerry <- tm_map(KatyPerry, removeWords, stopwords("en"))
tm_map(KatyPerry, removeWords, c("false", "relnofollowtwitter", "href", "web", "clienta"))
wordcloud(KatyPerry, min.freq = 2, scale=c(6,0.5),colors=brewer.pal(8, "Accent"), random.order = FALSE, max.words = 220)
KatyPerry <- Corpus(VectorSource(clean_tweet))
KatyPerry <- tm_map(KatyPerry, content_transformer(tolower))
KatyPerry <- tm_map(KatyPerry, removeWords, stopwords("en"))
tm_map(KatyPerry, removeWords, c("false", "relnofollowtwitter", "href", "web", "clienta"))
wordcloud(KatyPerry, min.freq = 2, scale=c(6,0.5),colors=brewer.pal(8, "Accent"), random.order = FALSE, max.words = 220)
KatyPerry <- Corpus(VectorSource(clean_tweet))
KatyPerry <- tm_map(KatyPerry, content_transformer(tolower))
KatyPerry <- tm_map(KatyPerry, removeWords, stopwords("english"))
tm_map(KatyPerry, removeWords, c("false", "relnofollowtwitter", "href", "web", "clienta"))
wordcloud(KatyPerry, min.freq = 2, scale=c(6,0.5),colors=brewer.pal(8, "Accent"), random.order = FALSE, max.words = 220)
unclean_tweets <- KPerry.df
iconv(unclean_tweets, from="UTF-8", to="ASCII", sub="")
clean_tweet <-  str_replace_all(unclean_tweets,"[^[:graph:]]", " ")
clean_tweet <- gsub("[^[:alnum:]///' ]", "", clean_tweet)
clean_tweet = gsub("&amp", "", clean_tweet)
clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet)
clean_tweet <- tolower(clean_tweet)
clean_tweet = gsub("@\\w+", "", clean_tweet)
clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
clean_tweet = gsub("http\\w+", "", clean_tweet)
clean_tweet = gsub("[ \t]{2,}", "", clean_tweet)
clean_tweet = gsub("^\\s+|\\s+$", "", clean_tweet)
KatyPerry <- Corpus(VectorSource(clean_tweet))
KatyPerry <- tm_map(KatyPerry, content_transformer(tolower))
KatyPerry <- tm_map(KatyPerry, removeWords, stopwords("english"))
tm_map(KatyPerry, removeWords, c("false", "relnofollowtwitter", "href", "web", "clienta"))
wordcloud(KatyPerry, min.freq = 2, scale=c(6,0.5),colors=brewer.pal(8, "Accent"), random.order = FALSE, max.words = 220)
x <- searchTwitter('#machinelearning', n = 3200, lang = 'en', resultType = 'popular')
KPerryData <- twListToDF(x)
KPerry.df <- data.frame(KPerryData)
unclean_tweets <- data.frame()
unclean_tweets <- KPerry.df
iconv(unclean_tweets, from="UTF-8", to="ASCII", sub="")
clean_tweet <-  str_replace_all(unclean_tweets,"[^[:graph:]]", " ")
clean_tweet <- gsub("[^[:alnum:]///' ]", "", clean_tweet)
clean_tweet = gsub("&amp", "", clean_tweet)
clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet)
clean_tweet <- tolower(clean_tweet)
clean_tweet = gsub("@\\w+", "", clean_tweet)
clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
clean_tweet = gsub("http\\w+", "", clean_tweet)
clean_tweet = gsub("[ \t]{2,}", "", clean_tweet)
clean_tweet = gsub("^\\s+|\\s+$", "", clean_tweet)
KatyPerry <- Corpus(VectorSource(clean_tweet))
KatyPerry <- tm_map(KatyPerry, content_transformer(tolower))
KatyPerry <- tm_map(KatyPerry, removeWords, c(stopwords("english"), "false", "relnofollowtwitter", "href", "web", "clienta"))
wordcloud(KatyPerry, min.freq = 2, scale=c(6,0.5),colors=brewer.pal(8, "Accent"), random.order = FALSE, max.words = 220)
KatyPerry <- Corpus(VectorSource(clean_tweet))
KatyPerry <- tm_map(KatyPerry, content_transformer(tolower))
KatyPerry <- tm_map(KatyPerry, removeWords, c(stopwords("english"), "false", "true", "iphonea", "relnofollowtweetdecka", "relnofollowtwitter", "href", "web", "clienta"))
wordcloud(KatyPerry, min.freq = 2, scale=c(6,0.5),colors=brewer.pal(8, "Accent"), random.order = FALSE, max.words = 200)
wordcloud(KatyPerry, min.freq = 2, scale=c(4,0.5),colors=brewer.pal(8, "Accent"), random.order = FALSE, max.words = 200)
wordcloud(KatyPerry, min.freq = 2, scale=c(4,0.5),colors=brewer.pal(8, "Accent"), random.order = FALSE, max.words = 200)
wordcloud(KatyPerry, min.freq = 2, scale=c(4,0.5),colors=brewer.pal(8, "Accent"), random.order = FALSE, max.words = 100)
wordcloud(KatyPerry, min.freq = 2, scale=c(4,0.5),colors=brewer.pal(8, "Dark2"), random.order = FALSE, max.words = 100)
x <- searchTwitter('#trailhead', n = 3200, lang = 'en', resultType = 'popular')
KPerryData <- twListToDF(x)
KPerry.df <- data.frame(KPerryData)
unclean_tweets <- data.frame()
unclean_tweets <- KPerry.df
iconv(unclean_tweets, from="UTF-8", to="ASCII", sub="")
clean_tweet <-  str_replace_all(unclean_tweets,"[^[:graph:]]", " ")
clean_tweet <- gsub("[^[:alnum:]///' ]", "", clean_tweet)
clean_tweet = gsub("&amp", "", clean_tweet)
clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet)
clean_tweet <- tolower(clean_tweet)
clean_tweet = gsub("@\\w+", "", clean_tweet)
clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
clean_tweet = gsub("http\\w+", "", clean_tweet)
clean_tweet = gsub("[ \t]{2,}", "", clean_tweet)
clean_tweet = gsub("^\\s+|\\s+$", "", clean_tweet)
KatyPerry <- Corpus(VectorSource(clean_tweet))
KatyPerry <- tm_map(KatyPerry, content_transformer(tolower))
KatyPerry <- tm_map(KatyPerry, removeWords, c(stopwords("english"), "false", "true", "iphonea", "relnofollowtweetdecka", "relnofollowtwitter", "href", "web", "clienta"))
wordcloud(KatyPerry, min.freq = 2, scale=c(4,0.5),colors=brewer.pal(8, "Dark2"), random.order = FALSE, max.words = 100)
library(twitteR)
library(ROAuth)
library(stringr)
library(tm)
library(wordcloud)
consumer_key <- "mftPn6qtOfCL46fzJY3pTeVI4"
consumer_secret <- "x5D1VIlV1O4TkZPshj31j88CutI8X2eSfgryy0pdMpDJjUTcKK"
access_token <- "2924790128-WORx3u9bTwDqAYr6zSanDnDlafLaSHBdzVKYZxC"
access_secret <- "aMFTRClgP3Q2K0pJ21p1fromq1OWx4cS1hU6i4qJyqpZb"
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
#screenName <- c("katyperry", "justinbieber", "rihanna", "taylorswift13", "TheEllenShow")
#checkHandles <- lookupUsers(screenName)
#x <- userTimeline("katyperry",n=3200,includeRts = FALSE)
#x <- searchTwitter('#machinelearning', n = 3200, lang = 'en', resultType = 'popular')
x <- searchTwitter('#trailhead', n = 3200, lang = 'en', resultType = 'popular')
KPerryData <- twListToDF(x)
KPerry.df <- data.frame(KPerryData)
unclean_tweets <- data.frame()
unclean_tweets <- KPerry.df
iconv(unclean_tweets, from="UTF-8", to="ASCII", sub="")
clean_tweet <-  str_replace_all(unclean_tweets,"[^[:graph:]]", " ")
clean_tweet <- gsub("[^[:alnum:]///' ]", "", clean_tweet)
clean_tweet = gsub("&amp", "", clean_tweet)
clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet)
clean_tweet <- tolower(clean_tweet)
clean_tweet = gsub("@\\w+", "", clean_tweet)
clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
clean_tweet = gsub("http\\w+", "", clean_tweet)
clean_tweet = gsub("[ \t]{2,}", "", clean_tweet)
clean_tweet = gsub("^\\s+|\\s+$", "", clean_tweet)
KatyPerry <- Corpus(VectorSource(clean_tweet))
KatyPerry <- tm_map(KatyPerry, content_transformer(tolower))
KatyPerry <- tm_map(KatyPerry, removeWords, c(stopwords("english"), "false", "true", "iphonea", "relnofollowtweetdecka", "relnofollowtwitter", "href", "web", "clienta"))
wordcloud(KatyPerry, min.freq = 2, scale=c(4,0.5),colors=brewer.pal(8, "Dark2"), random.order = FALSE, max.words = 100)
library(twitteR)
library(ROAuth)
library(stringr)
library(tm)
library(wordcloud)
consumer_key <- "mftPn6qtOfCL46fzJY3pTeVI4"
consumer_secret <- "x5D1VIlV1O4TkZPshj31j88CutI8X2eSfgryy0pdMpDJjUTcKK"
access_token <- "2924790128-WORx3u9bTwDqAYr6zSanDnDlafLaSHBdzVKYZxC"
access_secret <- "aMFTRClgP3Q2K0pJ21p1fromq1OWx4cS1hU6i4qJyqpZb"
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
#x <- userTimeline("katyperry",n=3200,includeRts = FALSE)
#x <- searchTwitter('#machinelearning', n = 3200, lang = 'en', resultType = 'popular')
x <- searchTwitter('#trailhead', n = 3200, lang = 'en', resultType = 'popular')
x <- searchTwitter('#trailhead', n = 3200, lang = 'en', resultType = 'popular')
swirl()
library(swirl)
ls()
remove(list = ls())
swirl()
end()
list()
ls()
quit()
library(swirl)
swirl()
getwd()
ls()
x <- 9
ls()
list.files()
dir()
?list.files
args(list.files)
old.dir <- getwd()
testdir <- dir.create(getwd())
dir.create(testwd)
dir.create("testdir")
setwd(testdir)
args(setwd)
setwd("testdir")
file.create("mytest.R")
dir()
file.exists("mytest.R")
file.info("mytest.R")
args(file.rename)
file.rename("mytest.R", "mytest2.R")
file.copy("mytest2.R", "mytest3.R")
nxt()
file.path("mytest3.R")
file.path("mytest3.R", folder1, folder2)
file.path("mytest3.R", "folder1", "folder2")
file.path(folder1", "folder2")
file.path("folder1", "folder2")
?dir.create
dir.create(file.path("testdir2","testdir2", recursive=TRUE))
dir.create(file.path("testdir2","testdir2"), recursive=TRUE)
dir.create(file.path("testdir2","testdir3"), recursive=TRUE)
setwd(old.dir)
remove.packages(swirl)
?remove.packages
remove.packages(swirl)
remove.packages("swirl")
library(swirl)
library(twitteR)
library(ROAuth)
library(stringr)
library(tm)
library(wordcloud)
consumer_key <- "mftPn6qtOfCL46fzJY3pTeVI4"
consumer_secret <- "x5D1VIlV1O4TkZPshj31j88CutI8X2eSfgryy0pdMpDJjUTcKK"
access_token <- "2924790128-WORx3u9bTwDqAYr6zSanDnDlafLaSHBdzVKYZxC"
access_secret <- "aMFTRClgP3Q2K0pJ21p1fromq1OWx4cS1hU6i4qJyqpZb"
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
#screenName <- c("katyperry", "justinbieber", "rihanna", "taylorswift13", "TheEllenShow")
#checkHandles <- lookupUsers(screenName)
#x <- userTimeline("katyperry",n=3200,includeRts = FALSE)
#x <- searchTwitter('#machinelearning', n = 3200, lang = 'en', resultType = 'popular')
x <- searchTwitter('#trailhead', n = 3200, lang = 'en', resultType = 'popular')
HashTagData <- twListToDF(x)
HashTag.df <- data.frame(KPerryData)
unclean_tweets <- data.frame()
unclean_tweets <- HashTag.df
iconv(unclean_tweets, from="UTF-8", to="ASCII", sub="")
clean_tweet <-  str_replace_all(unclean_tweets,"[^[:graph:]]", " ")
clean_tweet <- gsub("[^[:alnum:]///' ]", "", clean_tweet)
clean_tweet = gsub("&amp", "", clean_tweet)
clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet)
clean_tweet <- tolower(clean_tweet)
clean_tweet = gsub("@\\w+", "", clean_tweet)
clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
clean_tweet = gsub("http\\w+", "", clean_tweet)
clean_tweet = gsub("[ \t]{2,}", "", clean_tweet)
clean_tweet = gsub("^\\s+|\\s+$", "", clean_tweet)
HashTag <- Corpus(VectorSource(clean_tweet))
HashTag <- tm_map(HashTag, content_transformer(tolower))
HashTag <- tm_map(HashTag, removeWords, c(stopwords("english"), "false", "true", "iphonea", "relnofollowtweetdecka", "relnofollowtwitter", "href", "web","clientaa", "clienta"))
wordcloud(HashTag, min.freq = 2, scale=c(4,0.5),colors=brewer.pal(8, "Set1"), random.order = FALSE, max.words = 100)
x <- searchTwitter('#trailhead', n = 3200, lang = 'en', resultType = 'popular')
install.packages('ggplot2', dependencies = TRUE)
install.packages('RColorBrewer', dependencies = TRUE)
install.packages('RColorBrewer', dependencies = TRUE)
install.packages("RColorBrewer", dependencies = TRUE)
x <- searchTwitter('#trailheadx', n = 3200, lang = 'en', resultType = 'popular')
library(twitteR)
library(ROAuth)
library(stringr)
library(tm)
library(wordcloud)
consumer_key <- "mftPn6qtOfCL46fzJY3pTeVI4"
consumer_secret <- "x5D1VIlV1O4TkZPshj31j88CutI8X2eSfgryy0pdMpDJjUTcKK"
access_token <- "2924790128-WORx3u9bTwDqAYr6zSanDnDlafLaSHBdzVKYZxC"
access_secret <- "aMFTRClgP3Q2K0pJ21p1fromq1OWx4cS1hU6i4qJyqpZb"
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
x <- searchTwitter('#trailheadx', n = 3200, lang = 'en', resultType = 'popular')
x <- searchTwitter('#trailheadx', n = 3200, lang = 'en', resultType = 'popular')
HashTagData <- twListToDF(x)
HashTag.df <- data.frame(KPerryData)
unclean_tweets <- data.frame()
unclean_tweets <- HashTag.df
iconv(unclean_tweets, from="UTF-8", to="ASCII", sub="")
clean_tweet <-  str_replace_all(unclean_tweets,"[^[:graph:]]", " ")
clean_tweet <- gsub("[^[:alnum:]///' ]", "", clean_tweet)
clean_tweet = gsub("&amp", "", clean_tweet)
clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet)
clean_tweet <- tolower(clean_tweet)
clean_tweet = gsub("@\\w+", "", clean_tweet)
clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
clean_tweet = gsub("http\\w+", "", clean_tweet)
clean_tweet = gsub("[ \t]{2,}", "", clean_tweet)
clean_tweet = gsub("^\\s+|\\s+$", "", clean_tweet)
HashTag <- Corpus(VectorSource(clean_tweet))
HashTag <- tm_map(HashTag, content_transformer(tolower))
HashTag <- tm_map(HashTag, removeWords, c(stopwords("english"), "false", "true", "iphonea", "relnofollowtweetdecka", "relnofollowtwitter", "href", "web","clientaa", "clienta"))
wordcloud(HashTag, min.freq = 2, scale=c(4,0.5),colors=brewer.pal(8, "Set1"), random.order = FALSE, max.words = 100)
x <- searchTwitter('#IoT', n = 3200, lang = 'en', resultType = 'popular')
HashTagData <- twListToDF(x)
HashTag.df <- data.frame(KPerryData)
unclean_tweets <- data.frame()
unclean_tweets <- HashTag.df
iconv(unclean_tweets, from="UTF-8", to="ASCII", sub="")
clean_tweet <-  str_replace_all(unclean_tweets,"[^[:graph:]]", " ")
clean_tweet <- gsub("[^[:alnum:]///' ]", "", clean_tweet)
clean_tweet = gsub("&amp", "", clean_tweet)
clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet)
clean_tweet <- tolower(clean_tweet)
clean_tweet = gsub("@\\w+", "", clean_tweet)
clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
clean_tweet = gsub("http\\w+", "", clean_tweet)
clean_tweet = gsub("[ \t]{2,}", "", clean_tweet)
clean_tweet = gsub("^\\s+|\\s+$", "", clean_tweet)
HashTag <- Corpus(VectorSource(clean_tweet))
HashTag <- tm_map(HashTag, content_transformer(tolower))
HashTag <- tm_map(HashTag, removeWords, c(stopwords("english"), "false", "true", "iphonea", "relnofollowtweetdecka", "relnofollowtwitter", "href", "web","clientaa", "clienta"))
wordcloud(HashTag, min.freq = 2, scale=c(4,0.5),colors=brewer.pal(8, "Set1"), random.order = FALSE, max.words = 100)
wordcloud(HashTag, min.freq = 3, scale=c(4,0.5),colors=brewer.pal(8, "Set1"), random.order = FALSE, max.words = 100)
wordcloud(HashTag, min.freq = 3, scale=c(4,0.5),colors=brewer.pal(8, "Set1"), random.order = FALSE, max.words = 10)
getwd()
setwd("C:\Users\dsilv\Desktop\Learning\Data Science\Data-Science\Data Analysis with R - Udacity")
setwd(C:\Users\dsilv\Desktop\Learning\Data Science\Data-Science\Data Analysis with R - Udacity)
setwd("C:\Users\dsilv\Desktop\Learning\Data Science\Data-Science\Data Analysis with R - Udacity")
setwd("\Desktop\Learning\Data Science\Data-Science\Data Analysis with R - Udacity")
setwd("C:\Users\dsilv\Desktop\Learning\Data Science\Data-Science\Data Analysis with R - Udacity")
setwd('C:\Users\dsilv\Desktop\Learning\Data Science\Data-Science\Data Analysis with R - Udacity')
setwd('/Desktop/Learning/Data Science/Data-Science/Data Analysis with R - Udacity')
setwd('C:/Users/dsilv/Desktop/Learning/Data Science/Data-Science/Data Analysis with R - Udacity')
statesInfo <- read.csv('stateData.csv')
View(statesInfo)
x <- subset(statesInfo, illiteracy <= 0.5)
head(x)
install.packages('knitr', dependencies = T)
library(knitr)
summary(mtcars)
efficient <- subset(cars, mpg >= 23)
efficient <- subset(mtcars, mpg >= 23)
str(efficient)
subset(mtcars, mpg > 30 & hp > 100)
subset(mtcars, mpg < 14 | disp > 390)
subset(mtcars, qsec <= 16.90)
lightCars <- subset(mtcars, wt < 2)
mtcars$year <- 1974
View(mtcars)
mtcars <- subset(mtcars, select = -year)
mtcars$year <- c(1973, 1974)
View(mtcars)
mtcars <- subset(mtcars, select = -year)
mtcars$wt
cond <- mtcars$wt < 3
cond
mtcars$weight_class <- ifelse(cond, 'light', 'average')
mtcars$weight_class
cond <- mtcars$wt > 3.5
mtcars$weight_class <- ifelse(cond, 'heavy', mtcars$weight_class)
mtcars$weight_class
rm(cond)
rm(efficient)
subset(mtcars, mpg >= 30 | hp < 60)
library(twitteR)
library(ROAuth)
library(stringr)
library(tm)
library(wordcloud)
consumer_key <- "mftPn6qtOfCL46fzJY3pTeVI4"
consumer_secret <- "x5D1VIlV1O4TkZPshj31j88CutI8X2eSfgryy0pdMpDJjUTcKK"
access_token <- "2924790128-WORx3u9bTwDqAYr6zSanDnDlafLaSHBdzVKYZxC"
access_secret <- "aMFTRClgP3Q2K0pJ21p1fromq1OWx4cS1hU6i4qJyqpZb"
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
x <- searchTwitter('#IoT', n = 3200, lang = 'en', resultType = 'popular')
HashTagData <- twListToDF(x)
HashTag.df <- data.frame(KPerryData)
unclean_tweets <- data.frame()
unclean_tweets <- HashTag.df
iconv(unclean_tweets, from="UTF-8", to="ASCII", sub="")
clean_tweet <-  str_replace_all(unclean_tweets,"[^[:graph:]]", " ")
clean_tweet <- gsub("[^[:alnum:]///' ]", "", clean_tweet)
clean_tweet = gsub("&amp", "", clean_tweet)
clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet)
clean_tweet <- tolower(clean_tweet)
clean_tweet = gsub("@\\w+", "", clean_tweet)
clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
clean_tweet = gsub("http\\w+", "", clean_tweet)
clean_tweet = gsub("[ \t]{2,}", "", clean_tweet)
clean_tweet = gsub("^\\s+|\\s+$", "", clean_tweet)
HashTag <- Corpus(VectorSource(clean_tweet))
HashTag <- tm_map(HashTag, content_transformer(tolower))
HashTag <- tm_map(HashTag, removeWords, c(stopwords("english"), "false", "true", "iphonea", "relnofollowtweetdecka", "relnofollowtwitter", "href", "web","clientaa", "clienta"))
wordcloud(HashTag, min.freq = 3, scale=c(4,0.5),colors=brewer.pal(8, "Set1"), random.order = FALSE, max.words = 100)
x <- searchTwitter('#GoBlue', n = 3200, lang = 'en', resultType = 'popular')
HashTagData <- twListToDF(x)
HashTag.df <- data.frame(HashTagData)
unclean_tweets <- data.frame()
unclean_tweets <- HashTag.df
iconv(unclean_tweets, from="UTF-8", to="ASCII", sub="")
clean_tweet <-  str_replace_all(unclean_tweets,"[^[:graph:]]", " ")
clean_tweet <- gsub("[^[:alnum:]///' ]", "", clean_tweet)
clean_tweet = gsub("&amp", "", clean_tweet)
clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet)
clean_tweet <- tolower(clean_tweet)
clean_tweet = gsub("@\\w+", "", clean_tweet)
clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
clean_tweet = gsub("http\\w+", "", clean_tweet)
clean_tweet = gsub("[ \t]{2,}", "", clean_tweet)
clean_tweet = gsub("^\\s+|\\s+$", "", clean_tweet)
HashTag <- Corpus(VectorSource(clean_tweet))
HashTag <- tm_map(HashTag, content_transformer(tolower))
HashTag <- tm_map(HashTag, removeWords, c(stopwords("english"), "false", "true", "iphonea", "relnofollowtweetdecka", "relnofollowtwitter", "href", "web","clientaa", "clienta"))
wordcloud(HashTag, min.freq = 3, scale=c(4,0.5),colors=brewer.pal(8, "Set1"), random.order = FALSE, max.words = 100)
wordcloud(HashTag, min.freq = 3, scale=c(4,0.5),colors=brewer.pal(8, "Set1"), random.order = FALSE, max.words = 200)
wordcloud(HashTag, min.freq = 3, scale=c(5,0.5),colors=brewer.pal(8, "Set1"), random.order = FALSE, max.words = 200)
wordcloud(HashTag, min.freq = 3, scale=c(5,0.4),colors=brewer.pal(8, "Set1"), random.order = FALSE, max.words = 200)
wordcloud(HashTag, min.freq = 3, scale=c(5,0.4),colors=brewer.pal(8, "Set1"), random.order = FALSE, max.words = 300)
x <- searchTwitter('#trailheadx', n = 3200, lang = 'en', resultType = 'popular')
HashTagData <- twListToDF(x)
HashTag.df <- data.frame(HashTagData)
unclean_tweets <- data.frame()
unclean_tweets <- HashTag.df
iconv(unclean_tweets, from="UTF-8", to="ASCII", sub="")
clean_tweet <-  str_replace_all(unclean_tweets,"[^[:graph:]]", " ")
clean_tweet <- gsub("[^[:alnum:]///' ]", "", clean_tweet)
clean_tweet = gsub("&amp", "", clean_tweet)
clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet)
clean_tweet <- tolower(clean_tweet)
clean_tweet = gsub("@\\w+", "", clean_tweet)
clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
clean_tweet = gsub("http\\w+", "", clean_tweet)
clean_tweet = gsub("[ \t]{2,}", "", clean_tweet)
clean_tweet = gsub("^\\s+|\\s+$", "", clean_tweet)
HashTag <- Corpus(VectorSource(clean_tweet))
HashTag <- tm_map(HashTag, content_transformer(tolower))
HashTag <- tm_map(HashTag, removeWords, c(stopwords("english"), "false", "true", "iphonea", "relnofollowtweetdecka", "relnofollowtwitter", "href", "web","clientaa", "clienta"))
wordcloud(HashTag, min.freq = 3, scale=c(5,0.4),colors=brewer.pal(8, "Set1"), random.order = FALSE, max.words = 300)
x <- searchTwitter('#IoT', n = 3200, lang = 'en', resultType = 'popular')
HashTagData <- twListToDF(x)
HashTag.df <- data.frame(HashTagData)
unclean_tweets <- data.frame()
unclean_tweets <- HashTag.df
iconv(unclean_tweets, from="UTF-8", to="ASCII", sub="")
clean_tweet <-  str_replace_all(unclean_tweets,"[^[:graph:]]", " ")
clean_tweet <- gsub("[^[:alnum:]///' ]", "", clean_tweet)
clean_tweet = gsub("&amp", "", clean_tweet)
clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet)
clean_tweet <- tolower(clean_tweet)
clean_tweet = gsub("@\\w+", "", clean_tweet)
clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
clean_tweet = gsub("http\\w+", "", clean_tweet)
clean_tweet = gsub("[ \t]{2,}", "", clean_tweet)
clean_tweet = gsub("^\\s+|\\s+$", "", clean_tweet)
HashTag <- Corpus(VectorSource(clean_tweet))
HashTag <- tm_map(HashTag, content_transformer(tolower))
HashTag <- tm_map(HashTag, removeWords, c(stopwords("english"), "false", "true", "iphonea", "relnofollowtweetdecka", "relnofollowtwitter", "href", "web","clientaa", "clienta"))
wordcloud(HashTag, min.freq = 3, scale=c(5,0.4),colors=brewer.pal(8, "Set1"), random.order = FALSE, max.words = 300)
wordcloud(HashTag, min.freq = 3, scale=c(5,0.4),colors=brewer.pal(8, "Set1"), random.order = FALSE, max.words = 200)
wordcloud(HashTag, min.freq = 3, scale=c(5,0.2),colors=brewer.pal(8, "Set1"), random.order = FALSE, max.words = 200)
wordcloud(HashTag, min.freq = 3, scale=c(5,0.6),colors=brewer.pal(8, "Set1"), random.order = FALSE, max.words = 200)
wordcloud(HashTag, min.freq = 2, scale=c(5,0.6),colors=brewer.pal(8, "Set1"), random.order = FALSE, max.words = 200)
wordcloud(HashTag, min.freq = 1, scale=c(5,0.6),colors=brewer.pal(8, "Set1"), random.order = FALSE, max.words = 200)
wordcloud(HashTag, min.freq = 1, scale=c(6,0.6),colors=brewer.pal(8, "Set1"), random.order = FALSE, max.words = 200)
HashTag <- tm_map(HashTag, removeWords, c(stopwords("english"), "cfalse", "true", "iphonea", "relnofollowtweetdecka", "relnofollowtwitter", "href", "web","clientaa", "clienta"))
wordcloud(HashTag, min.freq = 1, scale=c(6,0.6),colors=brewer.pal(8, "Set1"), random.order = FALSE, max.words = 200)
x <- userTimeline("dsilvadeepal",n=3200,includeRts = FALSE)
HashTagData <- twListToDF(x)
HashTag.df <- data.frame(HashTagData)
unclean_tweets <- data.frame()
unclean_tweets <- HashTag.df
iconv(unclean_tweets, from="UTF-8", to="ASCII", sub="")
clean_tweet <-  str_replace_all(unclean_tweets,"[^[:graph:]]", " ")
clean_tweet <- gsub("[^[:alnum:]///' ]", "", clean_tweet)
clean_tweet = gsub("&amp", "", clean_tweet)
clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet)
clean_tweet <- tolower(clean_tweet)
clean_tweet = gsub("@\\w+", "", clean_tweet)
clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
clean_tweet = gsub("http\\w+", "", clean_tweet)
clean_tweet = gsub("[ \t]{2,}", "", clean_tweet)
clean_tweet = gsub("^\\s+|\\s+$", "", clean_tweet)
HashTag <- Corpus(VectorSource(clean_tweet))
HashTag <- tm_map(HashTag, content_transformer(tolower))
HashTag <- tm_map(HashTag, removeWords, c(stopwords("english"), "cfalse", "true", "iphonea", "relnofollowtweetdecka", "relnofollowtwitter", "href", "web","clientaa", "clienta"))
wordcloud(HashTag, min.freq = 1, scale=c(6,0.6),colors=brewer.pal(8, "Set1"), random.order = FALSE, max.words = 200)
wordcloud(HashTag, min.freq = 1, scale=c(6,0.6),colors=brewer.pal(8, "Set1"), random.order = FALSE, max.words = 100)
wordcloud(HashTag, min.freq = 1, scale=c(6,0.6),colors=brewer.pal(8, "Set1"), random.order = FALSE, max.words = 150)
HashTag <- tm_map(HashTag, removeWords, c(stopwords("english"), "false", "true", "iphonea", "relnofollowtweetdecka", "relnofollowtwitter", "href", "web","clientaa", "clienta"))
HashTag <- tm_map(HashTag, removeWords, c(stopwords("english"), "false", "true", "iphonea", "relnofollowtweetdecka", "relnofollowtwitter", "href", "web","clientaa", "clienta"))
wordcloud(HashTag, min.freq = 1, scale=c(6,0.6),colors=brewer.pal(8, "Set1"), random.order = FALSE, max.words = 150)
reddit <-  read.csv("reddit.csv")
reddit <-  read.csv("reddit.csv")
str(reddit)
table(reddit$employment.status)
summary(reddit)
levels(reddit$age.range)
library(ggplot2)
qplot(data=reddit, x=age.range)
levels(reddit$children)
levels(reddit$military.service)
?factor
x <- userTimeline("TheEllenShow",n=3200,includeRts = FALSE)
HashTagData <- twListToDF(x)
HashTag.df <- data.frame(HashTagData)
unclean_tweets <- data.frame()
unclean_tweets <- HashTag.df
iconv(unclean_tweets, from="UTF-8", to="ASCII", sub="")
clean_tweet <-  str_replace_all(unclean_tweets,"[^[:graph:]]", " ")
clean_tweet <- gsub("[^[:alnum:]///' ]", "", clean_tweet)
clean_tweet = gsub("&amp", "", clean_tweet)
clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet)
clean_tweet <- tolower(clean_tweet)
clean_tweet = gsub("@\\w+", "", clean_tweet)
clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
clean_tweet = gsub("http\\w+", "", clean_tweet)
clean_tweet = gsub("[ \t]{2,}", "", clean_tweet)
clean_tweet = gsub("^\\s+|\\s+$", "", clean_tweet)
HashTag <- Corpus(VectorSource(clean_tweet))
HashTag <- tm_map(HashTag, content_transformer(tolower))
HashTag <- tm_map(HashTag, removeWords, c(stopwords("english"), "false", "true", "iphonea", "relnofollowtweetdecka", "relnofollowtwitter", "href", "web","clientaa", "clienta"))
wordcloud(HashTag, min.freq = 1, scale=c(6,0.6),colors=brewer.pal(8, "Set1"), random.order = FALSE, max.words = 150)
HashTag <- tm_map(HashTag, removeWords, c(stopwords("english"), "false", "true", "iphonea", "relnofollow", "relnofollowtwitter", "href", "web","clientaa", "clienta"))
wordcloud(HashTag, min.freq = 1, scale=c(6,0.6),colors=brewer.pal(8, "Set1"), random.order = FALSE, max.words = 150)
HashTag <- tm_map(HashTag, removeWords, c(stopwords("english"), "false", "true", "iphonea", "relnofollowmedia", "relnofollowtwitter", "href", "web","clientaa", "clienta"))
wordcloud(HashTag, min.freq = 1, scale=c(6,0.6),colors=brewer.pal(8, "Set1"), random.order = FALSE, max.words = 150)
reddit$age.range <- ordered(reddit$age.range, levels = c("Under 18", "18-24", "25-34", "35-44", "45-54", "55-64", "65 or Above"))
qplot(data=reddit, x=age.range)
x <- searchTwitter('#Trailheadx', n = 3200, lang = 'en', resultType = 'popular')
HashTagData <- twListToDF(x)
HashTag.df <- data.frame(HashTagData)
unclean_tweets <- data.frame()
unclean_tweets <- HashTag.df
iconv(unclean_tweets, from="UTF-8", to="ASCII", sub="")
clean_tweet <-  str_replace_all(unclean_tweets,"[^[:graph:]]", " ")
clean_tweet <- gsub("[^[:alnum:]///' ]", "", clean_tweet)
clean_tweet = gsub("&amp", "", clean_tweet)
clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet)
clean_tweet <- tolower(clean_tweet)
clean_tweet = gsub("@\\w+", "", clean_tweet)
clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
clean_tweet = gsub("http\\w+", "", clean_tweet)
clean_tweet = gsub("[ \t]{2,}", "", clean_tweet)
clean_tweet = gsub("^\\s+|\\s+$", "", clean_tweet)
HashTag <- Corpus(VectorSource(clean_tweet))
HashTag <- tm_map(HashTag, content_transformer(tolower))
HashTag <- tm_map(HashTag, removeWords, c(stopwords("english"), "false", "true", "iphonea", "relnofollowmedia", "relnofollowtwitter", "href", "web","clientaa", "clienta"))
wordcloud(HashTag, min.freq = 1, scale=c(6,0.6),colors=brewer.pal(8, "Set1"), random.order = FALSE, max.words = 150)
HashTag <- tm_map(HashTag, removeWords, c(stopwords("english"), "cfalse", "true", "iphonea", "relnofollowmedia", "relnofollowtwitter", "href", "web","clientaa", "clienta"))
wordcloud(HashTag, min.freq = 1, scale=c(6,0.6),colors=brewer.pal(8, "Set1"), random.order = FALSE, max.words = 150)
